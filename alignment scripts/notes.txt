Mike stuff:

To the extent that you have remaining GRA time available, it would be great to look into the questions listed below.  Knowing the answers to these questions could be useful for expanding upon or resubmitting the ACL submission as well as the project’s final report.

 

-What is the effect of data augmentation on rare label accuracy?
-How many of the generated paraphrases are estimated to be domain- or task-specific, not appearing in WordNet or PPDB?
-Does it help to begin by training the CNN over a uniform distribution then freezing the filter weights before continuing with sampling based on the actual label distribution?
-Does it help to progressively increase the sampling proportion until max validation accuracy is achieved?
-How well do the previous paraphrases (from BEA-18 paper, incl NMT backtranslation) work using Bert-based ranking and sampling-based training?
-Are the previous paraphrases and the current ones (derived from the dataset itself) complementary for downstream performance improvement?
 

Note re 5 that WordNet does have glosses for synsets that could in principle be used with our four-sentence method; otherwise we’d only have the two sentence available.

###
Besides the questions below, I’d also be interested in hearing what you think we could’ve done differently to avoid deadline madness.  I saw a twitter thread on this topic (started by Sam Bowman) but was not particularly convinced by any of the replies.
###

#########################################################

For pipline

  cut -f 1-73 FILE
  python sep_score.py FILE NEW_DIR


For sanity
	./score.local.py ../data/GoogleNews-vectors-negative300.bin ../data/glove.6B.300d.txt.word2vec ../data/lexsub.tsv ../data/corrected.tsv ../data/gigaword4.5g.kenlm.bin
	./normalize.py scored.tsv  testing
	%s/\t/ /g *.tsvs
	cat anno tsv > feats
	cat *.feats | sort -R > maxent.feats
	then run score_ablation.ipynb

./score.sanity.py ../data/GoogleNews-vectors-negative300.bin ../data/glove.6B.300d.txt.word2vec ../data/multiple.cleaned.tsv ../data/corrected.tsv ../data/gigaword4.5g.kenlm.bin
mv scored.tsv multiple.scored.tsv

./score.sanity.py ../data/GoogleNews-vectors-negative300.bin ../data/glove.6B.300d.txt.word2vec ../data/lexsub.tsv ../data/corrected.tsv ../data/gigaword4.5g.kenlm.bin
mv scored.tsv lexsub.scored.tsv


 cut -f 1-40 lexsub.scored.tsv | perl -pe 's/\t/ /g' | grep -v "glove_src_para_sim" > lexsub.feats.tsv
 cut -f 1-40 multiple.scored.tsv | perl -pe 's/\t/ /g' | grep -v "glove_src_para_sim" > multiple.feats.tsv

 cut -f 51 multiple.scored.tsv | egrep -v "^\t" > multiple.anno.tsv
 cut -f 53 lexsub.scored.tsv | egrep -v "^\t"  > lexsub.anno.tsv

paste lexsub.anno.tsv lexsub.feats.tsv > lexsub.ready.tsv
paste multiple.anno.tsv multiple.feats.tsv > multiple.ready.tsv

cat lexsub.ready.tsv multiple.ready.tsv > sanity.feats

python score_noclass.py > sanity_output.tsv


